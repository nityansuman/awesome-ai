{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Layers and Models via Subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:55:37.659260Z",
     "start_time": "2021-02-09T16:55:36.056263Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import packages\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "tf.__version__ # 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Layer Class\n",
    "\n",
    "One of the central abstraction in TF is the Layer class.\n",
    "A layer encapsulates both a state (the layer's \"weights\") and a transformation from inputs to outputs (a \"call\", the layer's forward pass).\n",
    "\n",
    "Weights are created using `Variable` method and then passing a initializer to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:55:37.665338Z",
     "start_time": "2021-02-09T16:55:37.660806Z"
    }
   },
   "outputs": [],
   "source": [
    "# Linear dense (without any activation) layer implementation\n",
    "class Linear(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        # Initialize weights\n",
    "        w_init = tf.random_uniform_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"), trainable=True,\n",
    "        )\n",
    "        # Initialize bias\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Forward pass\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:55:38.464059Z",
     "start_time": "2021-02-09T16:55:37.666937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Linear at 0x7f4b985e2790>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Linear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would use a layer by calling it on some input tensor, much like a Python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:55:38.701021Z",
     "start_time": "2021-02-09T16:55:38.465123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tf.Tensor(\n",
      "[[ 0.01658548  0.05650462 -0.00680071  0.01357207  0.01159811 -0.04614431\n",
      "   0.00019255 -0.01403505 -0.05581309 -0.03841535 -0.06635642 -0.01357264\n",
      "  -0.01232324 -0.06170757 -0.08078814  0.00687404 -0.01288105 -0.00847041\n",
      "  -0.00017538  0.02897899 -0.0028989   0.01902173  0.02617068  0.01602283\n",
      "  -0.00087059 -0.05656181 -0.06385028 -0.06543048  0.01861737 -0.00904342\n",
      "   0.04498125 -0.04483186]\n",
      " [ 0.01658548  0.05650462 -0.00680071  0.01357207  0.01159811 -0.04614431\n",
      "   0.00019255 -0.01403505 -0.05581309 -0.03841535 -0.06635642 -0.01357264\n",
      "  -0.01232324 -0.06170757 -0.08078814  0.00687404 -0.01288105 -0.00847041\n",
      "  -0.00017538  0.02897899 -0.0028989   0.01902173  0.02617068  0.01602283\n",
      "  -0.00087059 -0.05656181 -0.06385028 -0.06543048  0.01861737 -0.00904342\n",
      "   0.04498125 -0.04483186]], shape=(2, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = Linear(input_dim=2)\n",
    "y = linear_layer(tf.ones((2, 2)))\n",
    "\n",
    "print(\"Output:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better way to create weights for each layer is by using `add_weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:55:38.707019Z",
     "start_time": "2021-02-09T16:55:38.702658Z"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        # Add weights in constructor\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_dim, units), initializer=tf.random_uniform_initializer(), trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(shape=(units,), initializer=tf.zeros_initializer(), trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:55:38.715174Z",
     "start_time": "2021-02-09T16:55:38.707989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Linear at 0x7f4b231c8a00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Linear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:55:38.724264Z",
     "start_time": "2021-02-09T16:55:38.716212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tf.Tensor(\n",
      "[[-0.06047176 -0.04816655 -0.03154886 -0.06251568  0.00958148 -0.01282692\n",
      "  -0.02240032  0.07374001 -0.0739876  -0.05883969  0.01308106 -0.01288871\n",
      "   0.04340095  0.03366411 -0.04783353 -0.04165464  0.01183216  0.03819682\n",
      "  -0.00598559  0.03709728 -0.06509309  0.00408354 -0.03959733 -0.07756019\n",
      "   0.02486961 -0.04697967 -0.05941094 -0.00075205 -0.00912856  0.01380936\n",
      "  -0.01735049  0.04662926]\n",
      " [-0.06047176 -0.04816655 -0.03154886 -0.06251568  0.00958148 -0.01282692\n",
      "  -0.02240032  0.07374001 -0.0739876  -0.05883969  0.01308106 -0.01288871\n",
      "   0.04340095  0.03366411 -0.04783353 -0.04165464  0.01183216  0.03819682\n",
      "  -0.00598559  0.03709728 -0.06509309  0.00408354 -0.03959733 -0.07756019\n",
      "   0.02486961 -0.04697967 -0.05941094 -0.00075205 -0.00912856  0.01380936\n",
      "  -0.01735049  0.04662926]], shape=(2, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = Linear(input_dim=2)\n",
    "y = linear_layer(tf.ones((2, 2)))\n",
    "\n",
    "print(\"Output:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers can have non-trainable weights as well. Set `trainable` to False for such weights.\n",
    "\n",
    "Such weights are ignored during backpropagation, when you are training the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:55:38.727899Z",
     "start_time": "2021-02-09T16:55:38.725501Z"
    }
   },
   "outputs": [],
   "source": [
    "class ComputeSum(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim=32):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        # Create non-trainable weights\n",
    "        self.total = self.add_weight(\n",
    "            shape=(input_dim,), initializer=tf.zeros_initializer(), trainable=False\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:55:38.734993Z",
     "start_time": "2021-02-09T16:55:38.728765Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ComputeSum at 0x7f4b2330cca0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ComputeSum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:55:38.744556Z",
     "start_time": "2021-02-09T16:55:38.736612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: <tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([2., 2.], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "sum_layer = ComputeSum(input_dim=2)\n",
    "y = sum_layer(tf.ones((2, 2)))\n",
    "\n",
    "print(\"Output:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like trainable weights it is part of layer.weights, but it gets categorized as a non-trainable weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:55:38.747907Z",
     "start_time": "2021-02-09T16:55:38.745276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([2., 2.], dtype=float32)>]\n",
      "Non-trainable weights: [<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([2., 2.], dtype=float32)>]\n",
      "Trainable_weights: []\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights:\", sum_layer.weights)\n",
    "print(\"Non-trainable weights:\", sum_layer.non_trainable_weights)\n",
    "print(\"Trainable_weights:\", sum_layer.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:55:38.758242Z",
     "start_time": "2021-02-09T16:55:38.748547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [<tf.Variable 'Variable:0' shape=(2, 32) dtype=float32, numpy=\n",
      "array([[-2.3279607e-02, -2.1544887e-02, -1.7158844e-02, -4.3387759e-02,\n",
      "        -9.9650621e-03,  2.9152036e-03, -2.1465087e-02,  4.6870921e-02,\n",
      "        -4.3387868e-02, -1.5940737e-02,  4.7146980e-02, -2.7294362e-02,\n",
      "        -3.2730326e-03, -1.1640072e-02, -4.0566910e-02, -2.0389259e-02,\n",
      "         7.3510781e-03,  4.2893652e-02,  2.2132624e-02, -5.2142739e-03,\n",
      "        -2.0865202e-02,  3.8359765e-02, -2.0445168e-02, -3.7553620e-02,\n",
      "         8.0000386e-03, -3.0260539e-02, -2.0804977e-02, -3.7090946e-02,\n",
      "        -6.4313412e-05,  3.5211667e-03,  2.5303986e-02,  4.0511042e-04],\n",
      "       [-3.7192155e-02, -2.6621664e-02, -1.4390014e-02, -1.9127918e-02,\n",
      "         1.9546542e-02, -1.5742123e-02, -9.3523413e-04,  2.6869085e-02,\n",
      "        -3.0599738e-02, -4.2898953e-02, -3.4065917e-02,  1.4405657e-02,\n",
      "         4.6673987e-02,  4.5304183e-02, -7.2666183e-03, -2.1265376e-02,\n",
      "         4.4810772e-03, -4.6968348e-03, -2.8118217e-02,  4.2311553e-02,\n",
      "        -4.4227887e-02, -3.4276225e-02, -1.9152164e-02, -4.0006567e-02,\n",
      "         1.6869571e-02, -1.6719125e-02, -3.8605966e-02,  3.6338892e-02,\n",
      "        -9.0642460e-03,  1.0288190e-02, -4.2654481e-02,  4.6224151e-02]],\n",
      "      dtype=float32)>, <tf.Variable 'Variable:0' shape=(32,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>]\n",
      "Non-trainable weights: []\n",
      "Trainable_weights: [<tf.Variable 'Variable:0' shape=(2, 32) dtype=float32, numpy=\n",
      "array([[-2.3279607e-02, -2.1544887e-02, -1.7158844e-02, -4.3387759e-02,\n",
      "        -9.9650621e-03,  2.9152036e-03, -2.1465087e-02,  4.6870921e-02,\n",
      "        -4.3387868e-02, -1.5940737e-02,  4.7146980e-02, -2.7294362e-02,\n",
      "        -3.2730326e-03, -1.1640072e-02, -4.0566910e-02, -2.0389259e-02,\n",
      "         7.3510781e-03,  4.2893652e-02,  2.2132624e-02, -5.2142739e-03,\n",
      "        -2.0865202e-02,  3.8359765e-02, -2.0445168e-02, -3.7553620e-02,\n",
      "         8.0000386e-03, -3.0260539e-02, -2.0804977e-02, -3.7090946e-02,\n",
      "        -6.4313412e-05,  3.5211667e-03,  2.5303986e-02,  4.0511042e-04],\n",
      "       [-3.7192155e-02, -2.6621664e-02, -1.4390014e-02, -1.9127918e-02,\n",
      "         1.9546542e-02, -1.5742123e-02, -9.3523413e-04,  2.6869085e-02,\n",
      "        -3.0599738e-02, -4.2898953e-02, -3.4065917e-02,  1.4405657e-02,\n",
      "         4.6673987e-02,  4.5304183e-02, -7.2666183e-03, -2.1265376e-02,\n",
      "         4.4810772e-03, -4.6968348e-03, -2.8118217e-02,  4.2311553e-02,\n",
      "        -4.4227887e-02, -3.4276225e-02, -1.9152164e-02, -4.0006567e-02,\n",
      "         1.6869571e-02, -1.6719125e-02, -3.8605966e-02,  3.6338892e-02,\n",
      "        -9.0642460e-03,  1.0288190e-02, -4.2654481e-02,  4.6224151e-02]],\n",
      "      dtype=float32)>, <tf.Variable 'Variable:0' shape=(32,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights:\", linear_layer.weights)\n",
    "print(\"Non-trainable weights:\", linear_layer.non_trainable_weights)\n",
    "print(\"Trainable_weights:\", linear_layer.trainable_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, you may not know in advance the size of your inputs, and you would like to lazily create weights when that value becomes known, some time after instantiating the layer.\n",
    "\n",
    "Use `build(self, inputs_shape)` method in such scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:55:38.762655Z",
     "start_time": "2021-02-09T16:55:38.759337Z"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Initialize weights here rather than in __init__\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_uniform\", trainable=False\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:55:38.768360Z",
     "start_time": "2021-02-09T16:55:38.763307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Linear at 0x7f4b231e2f70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Linear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __call__() method of your layer will automatically run build the first time it is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:55:38.778885Z",
     "start_time": "2021-02-09T16:55:38.769031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.12824091 -0.00382189  0.03500267 ...  0.0037626  -0.1274958\n",
      "  -0.03381208]\n",
      " [ 0.12824091 -0.00382189  0.03500267 ...  0.0037626  -0.1274958\n",
      "  -0.03381208]\n",
      " [ 0.12824091 -0.00382189  0.03500267 ...  0.0037626  -0.1274958\n",
      "  -0.03381208]\n",
      " ...\n",
      " [ 0.12824091 -0.00382189  0.03500267 ...  0.0037626  -0.1274958\n",
      "  -0.03381208]\n",
      " [ 0.12824091 -0.00382189  0.03500267 ...  0.0037626  -0.1274958\n",
      "  -0.03381208]\n",
      " [ 0.12824091 -0.00382189  0.03500267 ...  0.0037626  -0.1274958\n",
      "  -0.03381208]], shape=(100, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# At instantiation, we don't know on what inputs this is going to get called\n",
    "linear_layer = Linear(units=32)\n",
    "\n",
    "# The layer's weights are created dynamically the first time the layer is called\n",
    "y = linear_layer(tf.ones((100, 10)))\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers are recursively composable:\n",
    "\n",
    "- If you assign a Layer instance as attribute of another Layer, the outer layer will start tracking the weights of the inner layer.\n",
    "\n",
    "We recommend creating such sublayers in the __init__() method (since the sublayers will typically have a build method, they will be built when the outer layer gets built)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:55:38.783166Z",
     "start_time": "2021-02-09T16:55:38.779963Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLPBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        y = self.linear_3(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:55:38.798560Z",
     "start_time": "2021-02-09T16:55:38.784185Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: (100, 1)\n",
      "Weights: [<tf.Variable 'mlp_block/linear_6/Variable:0' shape=(64, 32) dtype=float32, numpy=\n",
      "array([[-0.04294861, -0.0325537 , -0.0386548 , ...,  0.04671087,\n",
      "         0.04081596, -0.01135286],\n",
      "       [-0.04653119,  0.03761765,  0.01638753, ..., -0.03778819,\n",
      "         0.01035241, -0.001336  ],\n",
      "       [ 0.02587212,  0.02146473, -0.02824861, ...,  0.01182736,\n",
      "         0.02844635,  0.00086957],\n",
      "       ...,\n",
      "       [ 0.02222217,  0.00842669,  0.02813211, ...,  0.00685203,\n",
      "        -0.04439139,  0.04384978],\n",
      "       [ 0.03513259,  0.00355319, -0.03237949, ..., -0.0266817 ,\n",
      "         0.00431416, -0.03424772],\n",
      "       [-0.04397074, -0.01329132, -0.02974968, ...,  0.02046299,\n",
      "         0.04075642, -0.0346027 ]], dtype=float32)>, <tf.Variable 'mlp_block/linear_7/Variable:0' shape=(32, 32) dtype=float32, numpy=\n",
      "array([[-0.00786608,  0.02778982, -0.04551524, ..., -0.01123388,\n",
      "        -0.04614855,  0.00516076],\n",
      "       [ 0.01132749,  0.00665132, -0.04467681, ...,  0.02743456,\n",
      "        -0.00145255, -0.01982683],\n",
      "       [ 0.02271812,  0.00397509,  0.02778986, ...,  0.04620935,\n",
      "        -0.03179287, -0.01454026],\n",
      "       ...,\n",
      "       [-0.034633  ,  0.02028779, -0.01042308, ...,  0.02886245,\n",
      "        -0.03654666,  0.00013968],\n",
      "       [-0.03636064,  0.00042161,  0.01453464, ..., -0.04318468,\n",
      "         0.02807308,  0.03515068],\n",
      "       [-0.01549579,  0.02952437, -0.01541675, ...,  0.03230149,\n",
      "         0.02385091,  0.02364881]], dtype=float32)>, <tf.Variable 'mlp_block/linear_8/Variable:0' shape=(32, 1) dtype=float32, numpy=\n",
      "array([[ 0.01663883],\n",
      "       [-0.00771843],\n",
      "       [ 0.04197811],\n",
      "       [ 0.03045913],\n",
      "       [ 0.03911886],\n",
      "       [-0.04372134],\n",
      "       [-0.01144351],\n",
      "       [ 0.01022159],\n",
      "       [ 0.04986161],\n",
      "       [-0.02591937],\n",
      "       [ 0.02757673],\n",
      "       [ 0.03215703],\n",
      "       [ 0.02015606],\n",
      "       [ 0.02861034],\n",
      "       [ 0.00645145],\n",
      "       [ 0.00063379],\n",
      "       [ 0.02795489],\n",
      "       [-0.0167771 ],\n",
      "       [ 0.02836852],\n",
      "       [-0.03692418],\n",
      "       [ 0.0367667 ],\n",
      "       [ 0.00332669],\n",
      "       [ 0.04666162],\n",
      "       [ 0.02992592],\n",
      "       [ 0.00472205],\n",
      "       [ 0.02201228],\n",
      "       [-0.00909863],\n",
      "       [-0.02014481],\n",
      "       [-0.01482455],\n",
      "       [ 0.01223917],\n",
      "       [ 0.03163284],\n",
      "       [ 0.01564008]], dtype=float32)>, <tf.Variable 'mlp_block/linear_6/Variable:0' shape=(32,) dtype=float32, numpy=\n",
      "array([ 0.0462211 ,  0.02814675, -0.0301106 ,  0.02776645,  0.02751782,\n",
      "       -0.00352502,  0.04429523,  0.00800464, -0.04576616, -0.01817863,\n",
      "       -0.00289391, -0.0085196 , -0.0376024 , -0.00931698, -0.01766505,\n",
      "       -0.01129853, -0.00495256,  0.00283086, -0.04335206,  0.0292978 ,\n",
      "        0.01503197, -0.01035788,  0.01580348,  0.02738203, -0.02243145,\n",
      "        0.00302782, -0.00141885, -0.03529993, -0.00215894, -0.01353277,\n",
      "        0.00356663, -0.04863397], dtype=float32)>, <tf.Variable 'mlp_block/linear_7/Variable:0' shape=(32,) dtype=float32, numpy=\n",
      "array([-0.01808856,  0.00127438, -0.04892332,  0.00176666, -0.00310338,\n",
      "       -0.02079678, -0.01601563, -0.01215082,  0.00723513,  0.03014357,\n",
      "       -0.02792453,  0.01071488,  0.02989108,  0.02774275,  0.03266541,\n",
      "       -0.02764052,  0.01213805,  0.01246418, -0.02091935, -0.0248638 ,\n",
      "       -0.03561469, -0.00106959,  0.0441621 , -0.01540923, -0.0040601 ,\n",
      "       -0.02553859, -0.04685105, -0.00209284,  0.00144608, -0.04743103,\n",
      "       -0.02923206, -0.03102727], dtype=float32)>, <tf.Variable 'mlp_block/linear_8/Variable:0' shape=(1,) dtype=float32, numpy=array([-0.01948274], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "perceptron = MLPBlock()\n",
    "y = perceptron(tf.ones(shape=(100, 64)))\n",
    "\n",
    "print(\"Output:\", y.shape)\n",
    "print(\"Weights:\", perceptron.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers developed in such fashion (via subclassing) are not serializable.\n",
    "\n",
    "If you need your custom layers to be serializable, you can optionally implement a `get_config()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:55:38.802147Z",
     "start_time": "2021-02-09T16:55:38.799264Z"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Linear, self).get_config()\n",
    "        config.update({\"units\": self.units})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:55:38.810688Z",
     "start_time": "2021-02-09T16:55:38.802800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: <__main__.Linear object at 0x7f4b20037190>\n",
      "Config: {'name': 'linear_9', 'trainable': True, 'dtype': 'float32', 'units': 64}\n",
      "New Layer from config: <__main__.Linear object at 0x7f4b20037be0>\n"
     ]
    }
   ],
   "source": [
    "layer = Linear(64)\n",
    "print(\"Layers:\", layer)\n",
    "\n",
    "config = layer.get_config()\n",
    "print(\"Config:\", config)\n",
    "\n",
    "new_layer = Linear.from_config(config)\n",
    "print(\"New Layer from config:\", new_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differently Behaving Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some layers, in particular the `BatchNormalization` layer and the `Dropout` layer, have different behaviors during training and inference.\n",
    "\n",
    "For such layers, it is standard practice to expose a `training` (boolean) argument in the `call()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:55:38.815447Z",
     "start_time": "2021-02-09T16:55:38.811693Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sample dropout wrapper\n",
    "class CustomDropout(tf.keras.layers.Layer):\n",
    "    def __init__(self, rate, **kwargs):\n",
    "        super(CustomDropout, self).__init__(**kwargs)\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            return tf.nn.dropout(inputs, rate=self.rate)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:55:38.820662Z",
     "start_time": "2021-02-09T16:55:38.816450Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLPBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(1)\n",
    "        self.dp = CustomDropout(rate=0.2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dp(x)\n",
    "        y = self.linear_3(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:55:38.835867Z",
     "start_time": "2021-02-09T16:55:38.821684Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [<tf.Variable 'mlp_block_1/linear_10/Variable:0' shape=(64, 32) dtype=float32, numpy=\n",
      "array([[ 0.00480832, -0.04449831, -0.01868737, ..., -0.07568541,\n",
      "         0.01739467,  0.01585306],\n",
      "       [ 0.07346726, -0.05165542, -0.10224849, ...,  0.0011067 ,\n",
      "         0.02950659,  0.04581891],\n",
      "       [ 0.06077056, -0.03540228,  0.03681133, ...,  0.06726784,\n",
      "        -0.02320555, -0.00682611],\n",
      "       ...,\n",
      "       [ 0.02116216, -0.0135638 , -0.0766438 , ...,  0.03039101,\n",
      "        -0.08805507, -0.02326634],\n",
      "       [-0.00771108,  0.00853711, -0.04889629, ..., -0.00316886,\n",
      "         0.02050136,  0.0009126 ],\n",
      "       [ 0.0104132 , -0.06640428, -0.03596489, ..., -0.0627687 ,\n",
      "         0.01927026,  0.00334143]], dtype=float32)>, <tf.Variable 'mlp_block_1/linear_10/Variable:0' shape=(32,) dtype=float32, numpy=\n",
      "array([ 0.01551276, -0.09641021,  0.05902372, -0.01977053,  0.04370142,\n",
      "       -0.07039469,  0.01610832, -0.0211281 , -0.0957595 ,  0.03623869,\n",
      "       -0.1146992 , -0.05096183,  0.01770232,  0.08560682, -0.0181333 ,\n",
      "        0.12799577, -0.10650665, -0.04513618,  0.0863476 , -0.03128976,\n",
      "        0.02992592, -0.05867767, -0.09950986,  0.03892168,  0.00736891,\n",
      "       -0.01068136, -0.01547918,  0.13079058, -0.0922102 , -0.05419159,\n",
      "        0.02672205, -0.07021414], dtype=float32)>, <tf.Variable 'mlp_block_1/linear_11/Variable:0' shape=(32, 32) dtype=float32, numpy=\n",
      "array([[-0.03959743, -0.06742234, -0.00220483, ..., -0.00325374,\n",
      "        -0.05063913,  0.03547034],\n",
      "       [ 0.05483905,  0.05215374,  0.01628807, ...,  0.05542976,\n",
      "        -0.03598873, -0.05081126],\n",
      "       [-0.03547765, -0.06424092,  0.04219355, ..., -0.00444283,\n",
      "         0.10332598, -0.02887096],\n",
      "       ...,\n",
      "       [-0.01990415,  0.01221408, -0.0361345 , ...,  0.06528144,\n",
      "        -0.03831467, -0.07960948],\n",
      "       [ 0.04563599, -0.05306691, -0.04717755, ...,  0.00025511,\n",
      "        -0.03185152,  0.04158094],\n",
      "       [-0.0269067 , -0.05951485, -0.07141989, ...,  0.04865572,\n",
      "        -0.0045265 ,  0.0358373 ]], dtype=float32)>, <tf.Variable 'mlp_block_1/linear_11/Variable:0' shape=(32,) dtype=float32, numpy=\n",
      "array([ 6.8779560e-03,  5.2921791e-02, -2.4808981e-02,  1.2624069e-02,\n",
      "        1.8047450e-02,  1.8789990e-02,  1.0602094e-03,  2.3753727e-02,\n",
      "        5.8523463e-03,  5.1518727e-02, -5.7492498e-04,  3.3958714e-02,\n",
      "       -8.1850827e-02, -1.2022759e-01,  6.3257895e-02,  3.3866592e-02,\n",
      "        1.1761502e-02,  6.3823685e-02, -4.2210527e-02, -1.1291051e-05,\n",
      "       -3.1641707e-02,  1.9219058e-02,  2.5311965e-02,  4.1424390e-02,\n",
      "       -3.6248248e-02, -2.9221097e-02, -1.0064485e-02,  1.3306319e-02,\n",
      "        8.3638933e-03, -1.4230996e-02, -8.5381910e-02,  4.5268159e-02],\n",
      "      dtype=float32)>, <tf.Variable 'mlp_block_1/linear_12/Variable:0' shape=(32, 1) dtype=float32, numpy=\n",
      "array([[-0.13392065],\n",
      "       [ 0.12258512],\n",
      "       [-0.00811117],\n",
      "       [-0.0825715 ],\n",
      "       [ 0.00822924],\n",
      "       [-0.02187583],\n",
      "       [-0.08446951],\n",
      "       [ 0.01338254],\n",
      "       [-0.05522082],\n",
      "       [ 0.03837901],\n",
      "       [-0.03822923],\n",
      "       [ 0.0039491 ],\n",
      "       [-0.02525496],\n",
      "       [ 0.06423717],\n",
      "       [-0.07314025],\n",
      "       [-0.03168384],\n",
      "       [ 0.0644653 ],\n",
      "       [ 0.01316603],\n",
      "       [ 0.11935786],\n",
      "       [ 0.05733196],\n",
      "       [-0.01560116],\n",
      "       [-0.0032273 ],\n",
      "       [ 0.06019221],\n",
      "       [-0.0537396 ],\n",
      "       [ 0.04287405],\n",
      "       [ 0.05340058],\n",
      "       [ 0.03385036],\n",
      "       [-0.02261647],\n",
      "       [ 0.03954924],\n",
      "       [-0.0769942 ],\n",
      "       [ 0.08276372],\n",
      "       [ 0.024973  ]], dtype=float32)>, <tf.Variable 'mlp_block_1/linear_12/Variable:0' shape=(1,) dtype=float32, numpy=array([0.05859711], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "perceptron = MLPBlock()\n",
    "y = perceptron(tf.ones(shape=(100, 64)))\n",
    "\n",
    "print(\"Weights:\", perceptron.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other privileged argument supported by call() is the `mask_zero` argument.\n",
    "\n",
    "TF automatically passes the correct boolean in suhch cases to __call__() for layers that support it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Model class has the same API as Layer, with the following differences:\n",
    "\n",
    "- It exposes built-in training, evaluation, and prediction loops (model.fit(), model.evaluate(), model.predict()).\n",
    "- It exposes the list of its inner layers, via the model.layers property.\n",
    "- It exposes saving and serialization APIs (save(), save_weights()...)\n",
    "\n",
    "In general, you will use the Layer class to define inner computation blocks, and will use the Model class to define the outer model -- the object you will train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T17:01:27.152144Z",
     "start_time": "2021-02-09T17:01:27.148007Z"
    }
   },
   "outputs": [],
   "source": [
    "class SampleModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super(SampleModel, self).__init__()\n",
    "        self.b1 = MLPBlock()\n",
    "        self.b2 = MLPBlock()\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes)\n",
    "    \n",
    "    def call(self, model_input):\n",
    "        x = self.b1(model_input)\n",
    "        x = self.b2(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T17:01:27.489679Z",
     "start_time": "2021-02-09T17:01:27.470538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.SampleModel object at 0x7f4b231c8790>\n"
     ]
    }
   ],
   "source": [
    "model = SampleModel(num_classes=2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together: an end-to-end example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A Layer encapsulate a state (created in __init__() or build()) and some computation (defined in call()).\n",
    "- Layers can be recursively nested to create new, bigger computation blocks.\n",
    "- Layers can create and track losses (typically regularization losses) as well as metrics, via add_loss() and add_metric()\n",
    "- The outer container, the thing you want to train, is a Model. A Model is just like a Layer, but with added training and serialization utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T17:02:38.785516Z",
     "start_time": "2021-02-09T17:02:38.776697Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=32, intermediate_dim=64, name=\"encoder\", **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, intermediate_dim=64, name=\"decoder\", **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n",
    "        self.dense_output = layers.Dense(original_dim, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T17:02:46.032512Z",
     "start_time": "2021-02-09T17:02:46.027387Z"
    }
   },
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(tf.keras.Model):\n",
    "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_dim,\n",
    "        intermediate_dim=64,\n",
    "        latent_dim=32,\n",
    "        name=\"autoencoder\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # Add KL divergence regularization loss.\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
    "        )\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T17:03:52.781797Z",
     "start_time": "2021-02-09T17:03:41.874517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "step 0: mean loss = 0.3371\n",
      "step 100: mean loss = 0.1247\n",
      "step 200: mean loss = 0.0987\n",
      "step 300: mean loss = 0.0888\n",
      "step 400: mean loss = 0.0840\n",
      "step 500: mean loss = 0.0807\n",
      "step 600: mean loss = 0.0786\n",
      "step 700: mean loss = 0.0770\n",
      "step 800: mean loss = 0.0758\n",
      "step 900: mean loss = 0.0748\n",
      "Start of epoch 1\n",
      "step 0: mean loss = 0.0746\n",
      "step 100: mean loss = 0.0739\n",
      "step 200: mean loss = 0.0734\n",
      "step 300: mean loss = 0.0730\n",
      "step 400: mean loss = 0.0726\n",
      "step 500: mean loss = 0.0722\n",
      "step 600: mean loss = 0.0719\n",
      "step 700: mean loss = 0.0716\n",
      "step 800: mean loss = 0.0714\n",
      "step 900: mean loss = 0.0712\n"
     ]
    }
   ],
   "source": [
    "# Simple training loop on MNIST data\n",
    "original_dim = 784\n",
    "vae = VariationalAutoEncoder(original_dim, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "# Iterate over epochs.\n",
    "for epoch in range(epochs):\n",
    "    print(\"Start of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed = vae(x_batch_train)\n",
    "            # Compute reconstruction loss\n",
    "            loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "            loss += sum(vae.losses)  # Add KLD regularization loss\n",
    "\n",
    "        grads = tape.gradient(loss, vae.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "\n",
    "        loss_metric(loss)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(\"step %d: mean loss = %.4f\" % (step, loss_metric.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, it can be trained using the built-in training loops like any other model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T17:04:36.686855Z",
     "start_time": "2021-02-09T17:04:33.367803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0745\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.0676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4b105347f0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VariationalAutoEncoder(784, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl-env]",
   "language": "python",
   "name": "conda-env-dl-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
