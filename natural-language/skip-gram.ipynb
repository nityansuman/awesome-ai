{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:40.793094Z",
     "start_time": "2021-02-08T00:40:39.875264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import packages\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import tqdm\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "tf.__version__ # 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "SEED = 21\n",
    "NUM_NS = 4 # Negative sampling\n",
    "WINDOW_SIZE = 2\n",
    "BUFFER = 10000\n",
    "BUFFER_BATCH_SIZE = 1024\n",
    "VOCAB_SIZE = 1000\n",
    "EMBEDDING_DIM = 128\n",
    "SEQUENCE_LENGTH = 10\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "PATH_TO_FILE = tf.keras.utils.get_file(\"shakespeare\", \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Gram Model\n",
    "\n",
    "Word2Vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets.\n",
    "\n",
    "These papers proposed two methods for learning representations of words:\n",
    "\n",
    "- Continuous Bag-of-Words Model which predicts the middle word based on surrounding context words. The context consists of a few words before and after the current (middle) word. This architecture is called a bag-of-words model as the order of words in the context is not important.\n",
    "\n",
    "- Continuous Skip-gram Model which predict words within a certain range before and after the current word in the same sentence.\n",
    "\n",
    "A skip-gram model predicts the context (or neighbors) of a word, given the word itself.\n",
    "The model is trained on skip-grams, which are n-grams that allows tokens to be skipped.\n",
    "The context of a word can be representend using a pair of (`target_words`, `context_word`) where `context_word` appears in the neighboring context of the `target_word`.\n",
    "\n",
    "The `window_size` here determines the number of neighbors on either side of the `target_word` for context.\n",
    "\n",
    "The objective of the skip-gram model is to maximize the probability of predicting context words given the `target_word`.\n",
    "\n",
    "The computation of probability here would require to take softmax over the entire vocabulary which is often very large as any word from the vocabulary can be predicted for the given `target_word`. The `Noise Contrastive Estimation` loss function is an efficient approximation for a full softmax. Rather than taking the whole vocabulary as sample for probability distribution, negative sampling is used.\n",
    "\n",
    "The simplified negative sampling objective for a `target_word` is to distinguish the context words from number of n negative samples drawn from a noise distribution P(W) of words. That is, an approximation of softmax over the entire vocabulary is to pose the loss for a `target_word` as a classification problem between context word and number of n negative samples.\n",
    "\n",
    "A negative sample is defined as (`target_word`, `context_word`) pair such that the context word does not appear in the `window_size` of the `target_word`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:40.802207Z",
     "start_time": "2021-02-08T00:40:40.794429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n"
     ]
    }
   ],
   "source": [
    "with open(PATH_TO_FILE) as f: \n",
    "    lines = f.read().splitlines()\n",
    "    for line in lines[:20]:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:41.316658Z",
     "start_time": "2021-02-08T00:40:40.803694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: tf.Tensor(b'First Citizen:', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "text_ds = tf.data.TextLineDataset(PATH_TO_FILE).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
    "\n",
    "for text in text_ds.take(1):\n",
    "    print(\"Input text:\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:41.327018Z",
     "start_time": "2021-02-08T00:40:41.321104Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Standardize sentences\n",
    "# def standardize_sequences(input_data):\n",
    "#     input_data = tf.strings.lower(input_data)\n",
    "#     return tf.strings.regex_replace(input_data, pattern=f\"[{re.escape(string.punctuation)}]\", rewrite=\"\")\n",
    "\n",
    "# # Test\n",
    "# standardize_sequences(\"Hello, I'm nityan!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:42.136306Z",
     "start_time": "2021-02-08T00:40:41.328112Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a vectorization layer\n",
    "vectorization_layer = TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\", # Using default standardization\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    "    output_mode=\"int\"\n",
    ")\n",
    "\n",
    "# Learn vectorizer\n",
    "vectorization_layer.adapt(text_ds.batch(132))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:42.140453Z",
     "start_time": "2021-02-08T00:40:42.137348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'and', 'to']\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary\n",
    "vocabulary = vectorization_layer.get_vocabulary()\n",
    "print(vocabulary[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:42.179061Z",
     "start_time": "2021-02-08T00:40:42.141923Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorization_layer).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:43.896222Z",
     "start_time": "2021-02-08T00:40:42.180230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 32777\n",
      "First sequence: [ 89 270   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# Get sequences out\n",
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "\n",
    "print(\"Number of sequences:\", len(sequences))\n",
    "print(\"First sequence:\", sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:43.899268Z",
     "start_time": "2021-02-08T00:40:43.897197Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subsampling table\n",
    "subsampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:51.346370Z",
     "start_time": "2021-02-08T00:40:43.900715Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32777/32777 [00:09<00:00, 3613.76it/s]\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = list(), list(), list()\n",
    "\n",
    "for sequence in tqdm.tqdm(sequences):\n",
    "    # Get positive ngrams\n",
    "    positive_ngrams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "        sequence=sequence,\n",
    "        vocabulary_size=VOCAB_SIZE,\n",
    "        sampling_table=subsampling_table,\n",
    "        window_size=WINDOW_SIZE,\n",
    "        negative_samples=0\n",
    "    )\n",
    "\n",
    "    for target_w, context_w in positive_ngrams:\n",
    "        # Create context tensor\n",
    "        context_class = tf.expand_dims(tf.constant([context_w], dtype=\"int64\"), 1)\n",
    "\n",
    "        # Get negative contexts\n",
    "        negative_context_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "            true_classes=context_class,\n",
    "            num_true=1,\n",
    "            num_sampled=NUM_NS,\n",
    "            unique=True,\n",
    "            range_max=VOCAB_SIZE,\n",
    "            seed=SEED\n",
    "        )\n",
    "        negative_context_candidates = tf.expand_dims(negative_context_candidates, 1)\n",
    "\n",
    "        # Add negative and positive contexts together\n",
    "        context = tf.concat([context_class, negative_context_candidates], 0)\n",
    "\n",
    "        # Create label\n",
    "        label = tf.constant([1] + [0]*NUM_NS, dtype=\"int64\")\n",
    "\n",
    "        # Store\n",
    "        targets.append(target_w)\n",
    "        contexts.append(context)\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:51.349967Z",
     "start_time": "2021-02-08T00:40:51.347500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35215 35215 35215\n"
     ]
    }
   ],
   "source": [
    "print(len(targets), len(contexts), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:53.045124Z",
     "start_time": "2021-02-08T00:40:51.350974Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create optimized model input\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER).batch(BUFFER_BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Word2Vec model can be implemented as a classifier to distinguish between true context words from skip-grams and false context words obtained through negative sampling.\n",
    "\n",
    "You can perform a dot product between the embeddings of target and context words to obtain predictions for labels and compute loss against true labels in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:53.051209Z",
     "start_time": "2021-02-08T00:40:53.046355Z"
    }
   },
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_ns):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=1, name=\"w2v_embedding\")\n",
    "        self.context_embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=num_ns+1)\n",
    "        self.dots = tf.keras.layers.Dot(axes=(3,2))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "    \n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        we = self.target_embedding(target)\n",
    "        ce = self.context_embedding(context)\n",
    "        dots = self.dots([ce, we])\n",
    "        return self.flatten(dots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:53.079339Z",
     "start_time": "2021-02-08T00:40:53.051937Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "word2vec = Word2Vec(VOCAB_SIZE, EMBEDDING_DIM, NUM_NS)\n",
    "\n",
    "word2vec.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:59.221338Z",
     "start_time": "2021-02-08T00:40:53.080354Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 1/34 [..............................] - ETA: 0s - loss: 1.6092 - accuracy: 0.1982WARNING:tensorflow:From /home/nityan/anaconda3/envs/dl-env/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0035s vs `on_train_batch_end` time: 0.0304s). Check your callbacks.\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 1.6071 - accuracy: 0.2673\n",
      "Epoch 2/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 1.5883 - accuracy: 0.4857\n",
      "Epoch 3/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 1.5477 - accuracy: 0.5190\n",
      "Epoch 4/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 1.4909 - accuracy: 0.4916\n",
      "Epoch 5/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 1.4403 - accuracy: 0.4771\n",
      "Epoch 6/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 1.3975 - accuracy: 0.4846\n",
      "Epoch 7/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 1.3561 - accuracy: 0.4998\n",
      "Epoch 8/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 1.3153 - accuracy: 0.5180\n",
      "Epoch 9/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 1.2753 - accuracy: 0.5376\n",
      "Epoch 10/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 1.2363 - accuracy: 0.5569\n",
      "Epoch 11/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 1.1982 - accuracy: 0.5746\n",
      "Epoch 12/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 1.1609 - accuracy: 0.5927\n",
      "Epoch 13/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 1.1244 - accuracy: 0.6114\n",
      "Epoch 14/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 1.0888 - accuracy: 0.6291\n",
      "Epoch 15/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 1.0541 - accuracy: 0.6471\n",
      "Epoch 16/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 1.0204 - accuracy: 0.6625\n",
      "Epoch 17/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.9877 - accuracy: 0.6774\n",
      "Epoch 18/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.9561 - accuracy: 0.6912\n",
      "Epoch 19/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.9256 - accuracy: 0.7034\n",
      "Epoch 20/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.8962 - accuracy: 0.7157\n",
      "Epoch 21/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.8680 - accuracy: 0.7271\n",
      "Epoch 22/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.8409 - accuracy: 0.7384\n",
      "Epoch 23/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.8150 - accuracy: 0.7487\n",
      "Epoch 24/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.7903 - accuracy: 0.7578\n",
      "Epoch 25/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.7666 - accuracy: 0.7676\n",
      "Epoch 26/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.7441 - accuracy: 0.7764\n",
      "Epoch 27/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.7226 - accuracy: 0.7848\n",
      "Epoch 28/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.7021 - accuracy: 0.7927\n",
      "Epoch 29/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.6826 - accuracy: 0.7999\n",
      "Epoch 30/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.6641 - accuracy: 0.8068\n",
      "Epoch 31/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.6464 - accuracy: 0.8126\n",
      "Epoch 32/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.6296 - accuracy: 0.8184\n",
      "Epoch 33/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.6136 - accuracy: 0.8239\n",
      "Epoch 34/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.5985 - accuracy: 0.8298\n",
      "Epoch 35/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.5840 - accuracy: 0.8352\n",
      "Epoch 36/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.5702 - accuracy: 0.8393\n",
      "Epoch 37/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.5572 - accuracy: 0.8435\n",
      "Epoch 38/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.5447 - accuracy: 0.8477\n",
      "Epoch 39/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.5328 - accuracy: 0.8512\n",
      "Epoch 40/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.5216 - accuracy: 0.8551\n",
      "Epoch 41/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.5108 - accuracy: 0.8587\n",
      "Epoch 42/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.5006 - accuracy: 0.8614\n",
      "Epoch 43/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4908 - accuracy: 0.8640\n",
      "Epoch 44/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4815 - accuracy: 0.8666\n",
      "Epoch 45/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4726 - accuracy: 0.8687\n",
      "Epoch 46/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4641 - accuracy: 0.8709\n",
      "Epoch 47/50\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.4560 - accuracy: 0.8725\n",
      "Epoch 48/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4483 - accuracy: 0.8747\n",
      "Epoch 49/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4409 - accuracy: 0.8764\n",
      "Epoch 50/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.4339 - accuracy: 0.8780\n"
     ]
    }
   ],
   "source": [
    "# Log modelling with tensorboard for analysis and visualization\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "history = word2vec.fit(dataset, epochs=50, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:41:36.394535Z",
     "start_time": "2021-02-08T00:41:04.986861Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tensorboard for embedding visualization\n",
    "# ! tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:59.238698Z",
     "start_time": "2021-02-08T00:40:59.225202Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get weights from layer\n",
    "weights = word2vec.get_layer(\"w2v_embedding\").get_weights()[0]\n",
    "vocab = vectorization_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:40:59.318070Z",
     "start_time": "2021-02-08T00:40:59.240127Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create and save metadata files\n",
    "out_v = io.open(\"/tmp/vectors.tsv\", \"w\", encoding=\"utf-8\")\n",
    "out_m = io.open(\"/tmp/metadata.tsv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if  index == 0: continue # skip 0, it\"s padding.\n",
    "    vec = weights[index] \n",
    "    out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl-env]",
   "language": "python",
   "name": "conda-env-dl-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "147px",
    "width": "308px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
