# Import packages
import tensorflow as tf
from tensorflow.keras import layers


class AdditiveAttentionWrapper(tf.keras.layers.Layer):
	"""Wrapper to the Tensorflow's ``AdditiveAttention`` layer a.k.a
	Bahdanau Attention.

	Reference:
		- Ts: Maximum input sequence length.
		- Te: Number of units in encoder layer.
		- Tt: Maximum output sequence length.
		- Td: Number of units in decoder layer.
	"""

	def __init__(self, units=512, **kwargs):
		"""Constructor.

		Args:
			units (int): Number of nodes. Defaults to 512.
		"""
		super(AdditiveAttentionWrapper, self).__init__(
			name="AdditiveAttentionWrapper", **kwargs)
		self.W1 = layers.Dense(units, use_bias=False)
		self.W2 = layers.Dense(units, use_bias=False)
		self.attention = layers.AdditiveAttention()

	def call(self, query, value, mask):
		"""Forward pass over the layer.

		Args:
			query (tensor]): Query tensor (generated decoder)
				of shape [None, Tt, Td].
			value (tensor): Value tensor (generated by encoder)
				of shape [None, Ts, Te].
			mask (tensor): Value mask tensor of shape [None, Tv].

		Returns:
			tensor, tensor: Context vector of shape [None, Tt, Te] and
				attention weight of shape [None, Tt, Ts].
		"""
		# Weighted query
		w1_query = self.W1(query)

		# Weighted value i.e., key in attention terminology
		w2_key = self.W2(value)

		# Compute query and value mask
		query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)
		value_mask = mask

		# Process query using value and key
		# Get context vector and attention weights
		context_vector, attention_weights = self.attention(
			inputs=[w1_query, value, w2_key],
			mask=[query_mask, value_mask],
			return_attention_scores=True,
		)
		return context_vector, attention_weights


class AttentiveGRUDecoder(tf.keras.layers.Layer):
	"""GRU based attentive decoder layer.

	Reference:
		- Ts: Maximum input sequence length.
		- Te: Number of units in encoder layer.
		- Tt: Maximum output sequence length.
		- Td: Number of units in decoder layer.
		- Tovs: Output vocabulary size.
	"""

	def __init__(self, output_vocab_size, embedding_dim, units=512, **kwargs):
		"""Constructor.

		Args:
			output_vocab_size (int): Vocabulary size of the target language.
			embedding_dim (int): Embedding dimension.
			units (int, optional): Number of nodes. Defaults to 512.
		"""
		super(AttentiveGRUDecoder, self).__init__(
			name="AttentiveGRUDecoder", **kwargs)
		self.units = units
		self.output_vocab_size = output_vocab_size
		self.embedding_dim = embedding_dim

		# Embedding layer converts tokens to vectors
		self.embedding = layers.Embedding(
			self.output_vocab_size,
			embedding_dim)

		# GRU layer processes encoded vectors sequentially
		self.gru = layers.GRU(
			self.units,
			return_sequences=True,
			return_state=True,
			recurrent_initializer="glorot_uniform")

		# Attention layer
		self.attention = AdditiveAttentionWrapper(self.units)

		# Fully connected layer
		self.dense1 = layers.Dense(
			units,
			activation="tanh",
			use_bias=False)

		# Final fully connected layer
		self.fc = layers.Dense(self.output_vocab_size)

	def call(self, tokens, enc_output, mask, state=None):
		"""Forward pass over the decoder layer.

		Args:
			tokens (tensor): Tokens tensor of shape [None, Tt].
			enc_output (tensor): Encoder output tensor of shape [None, Ts, Te].
			mask (tensor): Mask tensor of shape [None, Ts]
			state (tensor, optional): Encoder final state of shape [None, Te]. Defaults to None.

		Returns:
			tensor, tensor, tensor: Logits computed by decoder of shape [None, Tt, Tovs],
				attention weights of shape [None, Tt, Td] and decoder final state of
				shape [None, Td].
		"""
		# Lookup embeddings
		vector = self.embedding(tokens)

		# Process one step using RNN
		rnn_output, state = self.gru(vector, initial_state=state)

		# Compute attention over rnn output
		context_vector, attention_weights = self.attention(
			query=rnn_output,
			value=enc_output,
			mask=mask)

		# Merge context vector with processed output from rnn
		context_vectors_merged = tf.concat(
			[context_vector, rnn_output], axis=-1)

		# Compute attention vector
		attention_vector = self.dense1(context_vectors_merged)

		# Generate logits
		logits = self.fc(attention_vector)

		# Return logits, attention weights and its final state
		return logits, attention_weights, state
